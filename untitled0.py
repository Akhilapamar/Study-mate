# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JlmBLcHXaUHGURJP1Sf_tRyIwm2e4mdh
"""

# -----------------------------------------------------
# üìå Install Dependencies
# -----------------------------------------------------
!pip install transformers pypdf torch accelerate gradio

from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
from pypdf import PdfReader
import torch
import gradio as gr

# -----------------------------------------------------
# üìå Load Model
# -----------------------------------------------------
model_name = "ibm-granite/granite-3.3-2b-instruct"

print("‚è≥ Downloading model...")

tokenizer = AutoTokenizer.from_pretrained(model_name)

model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.float16,
    device_map="auto"
)

qa_pipeline = pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer,
    max_new_tokens=300,
    temperature=0.2,
    top_p=0.95,
    repetition_penalty=1.1
)

print("‚úÖ Model Loaded Successfully!")


# -----------------------------------------------------
# üìå PDF Processing Helpers
# -----------------------------------------------------
def extract_pdf_text(pdf_file):
    if pdf_file is None:
        return ""

    reader = PdfReader(pdf_file)
    text = ""

    for page in reader.pages:
        extracted = page.extract_text()
        if extracted:
            text += extracted + "\n"

    return text


def chunk_text(text, chunk_size=1200):
    return [text[i:i + chunk_size] for i in range(0, len(text), chunk_size)]


def retrieve_chunks(question, chunks, top_k=2):
    q_words = question.lower().split()
    scored = []

    for chunk in chunks:
        score = sum(1 for w in q_words if w in chunk.lower())
        scored.append((score, chunk))

    scored.sort(key=lambda x: x[0], reverse=True)
    return [c for _, c in scored[:top_k]]


# -----------------------------------------------------
# üìå Main QA Function for Gradio
# -----------------------------------------------------
def answer_question(question, pdf):
    if not pdf:
        return "‚ö† Please upload a PDF first."

    pdf_text = extract_pdf_text(pdf)
    chunks = chunk_text(pdf_text)

    context = "\n\n".join(retrieve_chunks(question, chunks))

    prompt = f"""
You are an intelligent study assistant. Use ONLY the context below to answer.

CONTEXT:
{context}

QUESTION:
{question}

ANSWER:
"""

    response = qa_pipeline(prompt)[0]["generated_text"].split("ANSWER:")[-1].strip()

    return response


# -----------------------------------------------------
# üìå Gradio Interface
# -----------------------------------------------------
with gr.Blocks(title="Studymate PDF Q&A") as demo:
    gr.Markdown("# üìò Studymate ‚Äì Ask Questions About Your PDF")
    gr.Markdown("Upload a PDF, then ask any question about its content.")

    with gr.Row():
        pdf_input = gr.File(label="üìÑ Upload PDF", file_types=[".pdf"])

    question = gr.Textbox(label="‚ùì Your Question")
    answer = gr.Textbox(label="üí° Answer", lines=5)

    ask_button = gr.Button("Ask")

    ask_button.click(
        answer_question,
        inputs=[question, pdf_input],
        outputs=[answer]
    )

demo.launch()

